# FASE 8 - AnÃ¡lise de Texto

<div align="center">

**Sistema avanÃ§ado de anÃ¡lise de texto com NLP, extraÃ§Ã£o de entidades e visualizaÃ§Ãµes**

</div>

## ğŸ“… **InformaÃ§Ãµes da Fase**

| Item | DescriÃ§Ã£o |
|------|-----------|
| **Status** | âœ… ConcluÃ­da |
| **Data de ConclusÃ£o** | 16 de Fevereiro de 2026 |
| **Artefatos** | Value Objects, Analisador SpaCy, WordCloud, Casos de Uso |
| **Testes** | 5 cenÃ¡rios manuais |
| **DependÃªncias** | FASE 1, 2, 3, 4, spaCy, TextBlob, NLTK, WordCloud, Matplotlib |

---

## ğŸ¯ **Objetivo**

Implementar um sistema de anÃ¡lise de texto que:

- Extraia entidades nomeadas (pessoas, locais, organizaÃ§Ãµes)
- Calcule estatÃ­sticas textuais (palavras, frases, densidade lÃ©xica)
- Analise sentimentos (polaridade e subjetividade)
- Gere nuvens de palavras (wordcloud)
- Suporte mÃºltiplos idiomas (russo, inglÃªs)
- Permita anÃ¡lise individual e global do acervo
- Visualize resultados no terminal e em imagens

---

## ğŸ”§ **ResoluÃ§Ã£o de DependÃªncias**

### Desafios Enfrentados

A instalaÃ§Ã£o das dependÃªncias para anÃ¡lise de texto apresentou diversos desafios devido a incompatibilidades de versÃµes:

| Problema | Causa | SoluÃ§Ã£o |
|----------|-------|---------|
| `requires-python = ">=3.14"` | ConfiguraÃ§Ã£o incorreta no `pyproject.toml` | Ajustado para `>=3.12,<3.14` |
| NumPy 1.24.4 | NÃ£o suporta PEP 517 | SubstituÃ­do por NumPy 1.26.0 |
| spaCy e dependÃªncias | Incompatibilidade com Python 3.13 | Fixado Python <3.13 |
| `smart-open` | LimitaÃ§Ã£o de versÃ£o Python | InstalaÃ§Ã£o direta com pip |

### SoluÃ§Ã£o Final

Optou-se por instalar as dependÃªncias diretamente com `pip` dentro do ambiente virtual do Poetry:

```bash
# Ativar ambiente
poetry shell

# Instalar pacotes com versÃµes compatÃ­veis
pip install numpy==1.26.0
pip install spacy==3.7.5
pip install textblob nltk wordcloud matplotlib
pip install https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0.tar.gz
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0.tar.gz
python -m textblob.download_corpora
```

### VersÃµes Instaladas

| Pacote | VersÃ£o | Status |
|--------|--------|--------|
| Python | 3.12.2 | âœ… |
| NumPy | 1.26.0 | âœ… |
| spaCy | 3.7.5 | âœ… |
| Modelo Russo | ru_core_news_sm | âœ… |
| Modelo InglÃªs | en_core_web_sm | âœ… |
| TextBlob | - | âœ… |
| NLTK | 3.9.2 | âœ… |
| WordCloud | 1.9.6 | âœ… |
| Matplotlib | 3.10.8 | âœ… |

---

## ğŸ“ **Estrutura Criada**

```
src/
â”œâ”€â”€ domain/
â”‚   â””â”€â”€ value_objects/
â”‚       â””â”€â”€ analise_texto.py              # Value Objects para anÃ¡lise
â”œâ”€â”€ application/
â”‚   â””â”€â”€ use_cases/
â”‚       â”œâ”€â”€ analisar_texto.py              # AnÃ¡lise individual
â”‚       â””â”€â”€ analisar_acervo.py             # AnÃ¡lise em lote
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ analysis/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ spacy_analyzer.py               # IntegraÃ§Ã£o com SpaCy
â”‚       â””â”€â”€ wordcloud_generator.py          # Nuvem de palavras
â””â”€â”€ interface/
    â””â”€â”€ cli/
        â”œâ”€â”€ commands_analise.py             # Comandos de anÃ¡lise
        â””â”€â”€ presenters_analise.py            # VisualizaÃ§Ã£o de anÃ¡lises
```

---

## ğŸ§© **Componentes Implementados**

### 1. Value Objects para AnÃ¡lise (`domain/value_objects/analise_texto.py`)

**Responsabilidade:** Representar os resultados da anÃ¡lise de texto.

```python
@dataclass
class Entidade:
    """Entidade extraÃ­da do texto."""
    texto: str
    tipo: str  # PERSON, ORG, LOC, DATE, etc
    confianca: float
    posicao_inicio: int
    posicao_fim: int


@dataclass
class Sentimento:
    """AnÃ¡lise de sentimento do texto."""
    polaridade: float  # -1 (negativo) a 1 (positivo)
    subjetividade: float  # 0 (objetivo) a 1 (subjetivo)
    classificacao: str  # positivo, negativo, neutro


@dataclass
class EstatisticasTexto:
    """EstatÃ­sticas bÃ¡sicas do texto."""
    total_caracteres: int
    total_palavras: int
    total_paragrafos: int
    total_frases: int
    palavras_unicas: int
    densidade_lexica: float
    tamanho_medio_palavra: float
    tamanho_medio_frase: float


@dataclass
class AnaliseTexto:
    """Resultado completo da anÃ¡lise de um texto."""
    documento_id: int
    idioma: str
    data_analise: datetime
    estatisticas: EstatisticasTexto
    entidades: List[Entidade]
    entidades_por_tipo: Dict[str, List[str]]
    sentimento: Sentimento
    palavras_frequentes: List[tuple]
    modelo_utilizado: str
    tempo_processamento: float

    @property
    def resumo(self) -> str:
        """Resumo da anÃ¡lise."""
        return (f"Doc {self.documento_id} ({self.idioma}): "
                f"{self.estatisticas.total_palavras} palavras, "
                f"{len(self.entidades)} entidades, "
                f"sentimento {self.sentimento.classificacao}")
```

---

### 2. Analisador SpaCy (`infrastructure/analysis/spacy_analyzer.py`)

**Responsabilidade:** Integrar com spaCy para anÃ¡lise de texto multilÃ­ngue.

```python
class SpacyAnalyzer:
    """
    Analisador de texto usando SpaCy com lazy loading.
    Modelos sÃ£o carregados apenas quando necessÃ¡rios.
    """

    # Mapeamento de idiomas para modelos SpaCy
    MODELOS = {
        'ru': 'ru_core_news_sm',
        'en': 'en_core_web_sm',
    }

    # Mapeamento de tipos de entidade para portuguÃªs
    TIPOS_ENTIDADE = {
        'PERSON': 'Pessoa',
        'ORG': 'OrganizaÃ§Ã£o',
        'LOC': 'Local',
        'GPE': 'Local (paÃ­s/cidade)',
        'DATE': 'Data',
        'TIME': 'Hora',
        'MONEY': 'Valor',
        'PERCENT': 'Percentual',
        'FAC': 'InstalaÃ§Ã£o',
        'PRODUCT': 'Produto',
        'EVENT': 'Evento',
        'LAW': 'Lei',
    }

    def __init__(self):
        """Inicializa sem carregar modelos."""
        self._models = {}  # Cache de modelos carregados
        logger.info("SpacyAnalyzer inicializado (modelos sob demanda)")

    def _get_model(self, idioma: str):
        """Carrega modelo sob demanda."""
        if idioma not in self.MODELOS:
            raise ValueError(f"Idioma nÃ£o suportado: {idioma}")

        if idioma in self._models:
            return self._models[idioma]

        modelo_nome = self.MODELOS[idioma]
        logger.info(f"ğŸ”„ Carregando modelo: {modelo_nome}")

        start = time.time()
        modelo = spacy.load(modelo_nome)
        elapsed = time.time() - start

        self._models[idioma] = modelo
        logger.info(f"âœ… Modelo carregado em {elapsed:.2f}s")

        return modelo

    def _calcular_estatisticas(self, texto: str, doc) -> EstatisticasTexto:
        """Calcula estatÃ­sticas do texto."""
        palavras = [token.text for token in doc if not token.is_punct and not token.is_space]
        frases = list(doc.sents)

        return EstatisticasTexto(
            total_caracteres=len(texto),
            total_palavras=len(palavras),
            total_paragrafos=texto.count('\n') + 1,
            total_frases=len(frases),
            palavras_unicas=len(set(p.lower() for p in palavras)),
            densidade_lexica=len(set(palavras)) / len(palavras) if palavras else 0,
            tamanho_medio_palavra=sum(len(p) for p in palavras) / len(palavras) if palavras else 0,
            tamanho_medio_frase=len(palavras) / len(frases) if frases else 0
        )

    def _extrair_entidades(self, doc) -> List[Entidade]:
        """Extrai entidades do documento."""
        entidades = []
        for ent in doc.ents:
            entidades.append(Entidade(
                texto=ent.text,
                tipo=ent.label_,
                confianca=1.0,
                posicao_inicio=ent.start_char,
                posicao_fim=ent.end_char
            ))
        return entidades

    def _agrupar_entidades(self, entidades: List[Entidade]) -> Dict[str, List[str]]:
        """Agrupa entidades por tipo."""
        grupos = {}
        for ent in entidades:
            tipo_pt = self.TIPOS_ENTIDADE.get(ent.tipo, ent.tipo)
            if tipo_pt not in grupos:
                grupos[tipo_pt] = []
            if ent.texto not in grupos[tipo_pt]:
                grupos[tipo_pt].append(ent.texto)
        return grupos

    def _analisar_sentimento(self, texto: str, idioma: str) -> Sentimento:
        """AnÃ¡lise de sentimento bÃ¡sica com TextBlob."""
        try:
            from textblob import TextBlob

            if idioma == 'en':
                blob = TextBlob(texto)
                polaridade = blob.sentiment.polarity
                subjetividade = blob.sentiment.subjectivity
            else:
                # Para russo, placeholder
                polaridade = 0.0
                subjetividade = 0.5
        except ImportError:
            polaridade = 0.0
            subjetividade = 0.5

        if polaridade > 0.1:
            classificacao = 'positivo'
        elif polaridade < -0.1:
            classificacao = 'negativo'
        else:
            classificacao = 'neutro'

        return Sentimento(
            polaridade=polaridade,
            subjetividade=subjetividade,
            classificacao=classificacao
        )

    def _palavras_frequentes(self, doc, limite: int = 20) -> List[tuple]:
        """Extrai palavras mais frequentes."""
        palavras = [
            token.text.lower() for token in doc
            if not token.is_stop
            and not token.is_punct
            and not token.is_space
            and len(token.text) > 2
        ]

        contador = Counter(palavras)
        return contador.most_common(limite)

    def analisar(self,
                 texto: str,
                 documento_id: int,
                 idioma: str = 'ru') -> AnaliseTexto:
        """Analisa texto completo."""
        inicio = time.time()

        # Carregar modelo (lazy)
        nlp = self._get_model(idioma)

        # Processar texto
        doc = nlp(texto[:100000])

        # EstatÃ­sticas
        estatisticas = self._calcular_estatisticas(texto, doc)

        # Entidades
        entidades = self._extrair_entidades(doc)
        entidades_agrupadas = self._agrupar_entidades(entidades)

        # Sentimento
        sentimento = self._analisar_sentimento(texto, idioma)

        # Palavras frequentes
        palavras_freq = self._palavras_frequentes(doc)

        tempo = time.time() - inicio

        return AnaliseTexto(
            documento_id=documento_id,
            idioma=idioma,
            data_analise=datetime.now(),
            estatisticas=estatisticas,
            entidades=entidades,
            entidades_por_tipo=entidades_agrupadas,
            sentimento=sentimento,
            palavras_frequentes=palavras_freq,
            modelo_utilizado=f"spacy-{idioma}",
            tempo_processamento=tempo
        )
```

---

### 3. Gerador de Nuvem de Palavras (`infrastructure/analysis/wordcloud_generator.py`)

**Responsabilidade:** Gerar imagens de nuvem de palavras.

```python
class WordCloudGenerator:
    """
    Gerador de nuvens de palavras a partir de textos.
    """

    def __init__(self, **kwargs):
        """Inicializa com configuraÃ§Ãµes flexÃ­veis."""
        self.default_size = kwargs.get('default_size', (800, 400))
        self.max_words = kwargs.get('max_words', 200)
        self.background_color = kwargs.get('background_color', 'white')
        self.width = kwargs.get('width', self.default_size[0])
        self.height = kwargs.get('height', self.default_size[1])

        self.stopwords = self._carregar_stopwords()

    def _carregar_stopwords(self) -> set:
        """Carrega stopwords de vÃ¡rios idiomas."""
        stopwords = set()

        # Stopwords em russo
        stopwords_ru = {'Ğ¸', 'Ğ²', 'Ğ½Ğ°', 'Ñ', 'Ğ¿Ğ¾', 'Ğ´Ğ»Ñ', 'Ñ‡Ñ‚Ğ¾', 'ĞºĞ°Ğº', 'ÑÑ‚Ğ¾',
                        'Ğ²ĞµÑÑŒ', 'Ğ¼Ğ¾Ğ¹', 'Ñ‚Ğ²Ğ¾Ğ¹', 'ĞµĞ³Ğ¾', 'ĞµĞµ', 'Ğ¸Ñ…', 'Ğº', 'Ñƒ', 'Ğ¾',
                        'Ğ¸Ğ·', 'Ğ·Ğ°', 'Ğ½Ğ°Ğ´', 'Ğ¿Ğ¾Ğ´', 'Ğ°', 'Ğ½Ğ¾', 'Ğ´Ğ°', 'Ğ¸Ğ»Ğ¸', 'ĞµÑĞ»Ğ¸'}

        # Stopwords em inglÃªs
        try:
            from wordcloud import STOPWORDS
            stopwords.update(STOPWORDS)
        except:
            pass

        stopwords.update(stopwords_ru)
        return stopwords

    def gerar(self,
              texto: str,
              titulo: str = "Nuvem de Palavras",
              idioma: str = 'ru',
              max_palavras: Optional[int] = None,
              largura: Optional[int] = None,
              altura: Optional[int] = None,
              salvar_em: Optional[str] = None) -> Optional[Path]:
        """Gera nuvem de palavras a partir do texto."""
        max_words = max_palavras or self.max_words
        width = largura or self.width
        height = altura or self.height

        # Processar texto com SpaCy
        try:
            if idioma == 'ru':
                nlp = spacy.load('ru_core_news_sm')
            else:
                nlp = spacy.load('en_core_web_sm')

            doc = nlp(texto[:50000])

            palavras = [
                token.text.lower() for token in doc
                if not token.is_stop
                and not token.is_punct
                and not token.is_space
                and len(token.text) > 2
                and token.text.lower() not in self.stopwords
            ]

            frequencias = Counter(palavras)

        except Exception:
            # Fallback
            palavras = texto.lower().split()
            palavras = [p for p in palavras if len(p) > 2 and p not in self.stopwords]
            frequencias = Counter(palavras)

        if not frequencias:
            return None

        # Gerar nuvem
        wordcloud = WordCloud(
            width=width,
            height=height,
            background_color=self.background_color,
            max_words=max_words,
            stopwords=self.stopwords,
            collocations=False
        ).generate_from_frequencies(frequencias)

        # Plotar
        plt.figure(figsize=(width/100, height/100))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(titulo)

        if salvar_em:
            caminho = Path(salvar_em)
            caminho.parent.mkdir(exist_ok=True)
            plt.savefig(caminho, bbox_inches='tight', dpi=300)
            plt.close()
            return caminho
        else:
            plt.show()
            return None
```

---

### 4. Caso de Uso AnalisarDocumento (`application/use_cases/analisar_texto.py`)

**Responsabilidade:** Analisar um documento especÃ­fico.

```python
class AnalisarDocumento:
    """
    Caso de uso para analisar um documento.
    """

    def __init__(self,
                 repo_doc: RepositorioDocumento,
                 repo_trad: Optional[RepositorioTraducao] = None,
                 registry: Optional[ServiceRegistry] = None):
        self.repo_doc = repo_doc
        self.repo_trad = repo_trad
        self.registry = registry or ServiceRegistry()

    def _get_analyzer(self) -> SpacyAnalyzer:
        """ObtÃ©m analisador spaCy do registry."""
        return self.registry.get('spacy')

    def _get_wordcloud(self) -> WordCloudGenerator:
        """ObtÃ©m gerador de wordcloud do registry."""
        return self.registry.get('wordcloud')

    def executar(self,
                 documento_id: int,
                 idioma: str = 'ru',
                 gerar_wordcloud: bool = False) -> Optional[AnaliseTexto]:
        """
        Analisa um documento especÃ­fico.
        """
        # 1. Buscar texto
        if idioma == 'ru':
            doc = self.repo_doc.buscar_por_id(documento_id)
            if not doc:
                return None
            texto = doc.texto
        else:
            if not self.repo_trad:
                return None
            traducao = self.repo_trad.buscar_por_documento(documento_id, idioma)
            if not traducao:
                return None
            texto = traducao.texto_traduzido

        # 2. Analisar
        analyzer = self._get_analyzer()
        analise = analyzer.analisar(
            texto=texto,
            documento_id=documento_id,
            idioma=idioma
        )

        # 3. Gerar wordcloud se solicitado
        if gerar_wordcloud:
            wordcloud = self._get_wordcloud()
            nome_arquivo = f"wordcloud_{documento_id}_{idioma}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            caminho = Path("analises") / nome_arquivo
            wordcloud.gerar(
                texto=texto,
                titulo=f"Documento {documento_id} - {idioma}",
                idioma=idioma,
                salvar_em=str(caminho)
            )
            # Opcional: adicionar caminho Ã  anÃ¡lise
            # analise.wordcloud_path = str(caminho)

        return analise
```

---

### 5. Caso de Uso AnalisarAcervo (`application/use_cases/analisar_acervo.py`)

**Responsabilidade:** AnÃ¡lise global do acervo.

```python
class AnalisarAcervo:
    """
    Caso de uso para anÃ¡lise global do acervo.
    """

    def __init__(self,
                 repo_doc: RepositorioDocumento,
                 registry: Optional[ServiceRegistry] = None):
        self.repo_doc = repo_doc
        self.registry = registry or ServiceRegistry()

    def _get_analyzer(self) -> SpacyAnalyzer:
        """ObtÃ©m analisador spaCy do registry."""
        return self.registry.get('spacy')

    def _get_wordcloud(self) -> WordCloudGenerator:
        """ObtÃ©m gerador de wordcloud do registry."""
        return self.registry.get('wordcloud')

    def estatisticas_globais(self) -> Dict:
        """
        EstatÃ­sticas agregadas de todo o acervo.
        """
        documentos = self.repo_doc.listar(limite=5000)

        stats = {
            'total_docs': len(documentos),
            'total_palavras': 0,
            'total_caracteres': 0,
            'media_palavras_por_doc': 0,
            'documentos_por_tamanho': {
                'pequeno (<1000 palavras)': 0,
                'mÃ©dio (1000-5000 palavras)': 0,
                'grande (>5000 palavras)': 0
            },
            'pessoas_mais_citadas': Counter(),
            'top_locais': Counter(),
            'top_organizacoes': Counter(),
        }

        # Amostra para anÃ¡lise de entidades
        amostra = documentos[:100]

        for doc in amostra:
            palavras = doc.texto.split()
            num_palavras = len(palavras)
            stats['total_palavras'] += num_palavras
            stats['total_caracteres'] += len(doc.texto)

            if num_palavras < 1000:
                stats['documentos_por_tamanho']['pequeno (<1000 palavras)'] += 1
            elif num_palavras < 5000:
                stats['documentos_por_tamanho']['mÃ©dio (1000-5000 palavras)'] += 1
            else:
                stats['documentos_por_tamanho']['grande (>5000 palavras)'] += 1

        if stats['total_docs'] > 0:
            stats['media_palavras_por_doc'] = stats['total_palavras'] / stats['total_docs']

        return stats

    def gerar_wordcloud_geral(self, idioma: str = 'ru') -> Path:
        """
        Gera nuvem de palavras com todo o acervo.
        """
        documentos = self.repo_doc.listar(limite=500)

        texto_completo = ""
        for doc in documentos[:100]:
            texto_completo += doc.texto[:5000] + "\n"

        nome_arquivo = f"wordcloud_acervo_{idioma}_{datetime.now().strftime('%Y%m%d')}.png"
        caminho = Path("analises") / nome_arquivo

        wordcloud = self._get_wordcloud()
        wordcloud.gerar(
            texto=texto_completo,
            titulo=f"Acervo Completo - {idioma}",
            idioma=idioma,
            max_palavras=200,
            salvar_em=str(caminho)
        )

        return caminho
```

---

### 6. ComandoAnalisarDocumento (`interface/cli/commands_analise.py`)

**Responsabilidade:** Interface para anÃ¡lise individual.

```python
class ComandoAnalisarDocumento:
    """Comando para analisar um documento especÃ­fico."""

    def __init__(self, analisar_documento_use_case):
        self.analisar = analisar_documento_use_case
        self.presenter = AnalisePresenter()

    def executar(self, documento_id: int):
        """Executa anÃ¡lise interativa."""
        console.print("\n[bold cyan]ğŸ” ANÃLISE DE DOCUMENTO[/bold cyan]")

        # 1. Escolher idioma
        console.print("[bold]Idioma para anÃ¡lise:[/bold]")
        console.print("  [1] ğŸ‡·ğŸ‡º Original (Russo)")
        console.print("  [2] ğŸ‡ºğŸ‡¸ InglÃªs (se disponÃ­vel)")
        console.print("  [0] Cancelar")

        opcao = input("\nEscolha: ").strip()

        idiomas = {'1': 'ru', '2': 'en'}
        if opcao == '0':
            return
        if opcao not in idiomas:
            console.mostrar_erro("OpÃ§Ã£o invÃ¡lida!")
            return

        idioma = idiomas[opcao]

        # 2. Gerar wordcloud?
        console.print("\n[bold]Gerar nuvem de palavras?[/bold]")
        console.print("  [1] Sim")
        console.print("  [2] NÃ£o")

        opcao = input("\nEscolha: ").strip()
        gerar_wordcloud = (opcao == '1')

        # 3. Confirmar
        console.print(f"\n[bold]Analisando documento {documento_id}...[/bold]")
        console.print(f"  â€¢ Idioma: {idioma}")
        console.print(f"  â€¢ Nuvem: {'Sim' if gerar_wordcloud else 'NÃ£o'}")

        confirmar = input("\nConfirmar? (s/N): ").strip().lower()
        if confirmar != 's':
            return

        # 4. Analisar
        try:
            with console.status("[cyan]Processando texto..."):
                resultado = console.spinner(
                    "Analisando documento...",
                    self.analisar.executar,
                    documento_id,
                    idioma,
                    gerar_wordcloud
                )

            if resultado:
                self.presenter.mostrar_analise(resultado)
            else:
                console.mostrar_erro("Documento ou traduÃ§Ã£o nÃ£o encontrado!")

        except Exception as e:
            console.mostrar_erro(f"Erro na anÃ¡lise: {e}")

        input("\nPressione Enter para continuar...")
```

---

### 7. ComandoAnalisarAcervo (`interface/cli/commands_analise.py`)

**Responsabilidade:** Interface para anÃ¡lise global.

```python
class ComandoAnalisarAcervo:
    """Comando para anÃ¡lise global do acervo."""

    def __init__(self, analisar_acervo_use_case):
        self.analisar = analisar_acervo_use_case
        self.presenter = AnalisePresenter()

    def executar(self):
        """Executa anÃ¡lise global."""
        console.print("\n[bold cyan]ğŸ“Š ANÃLISE GLOBAL DO ACERVO[/bold cyan]")

        console.print("[bold]OpÃ§Ãµes:[/bold]")
        console.print("  [1] EstatÃ­sticas globais")
        console.print("  [2] Nuvem de palavras do acervo")
        console.print("  [0] Cancelar")

        opcao = input("\nEscolha: ").strip()

        if opcao == '0':
            return

        elif opcao == '1':
            with console.status("[cyan]Calculando estatÃ­sticas..."):
                stats = self.analisar.estatisticas_globais()
            self.presenter.mostrar_estatisticas_globais(stats)

        elif opcao == '2':
            console.print("\n[bold]Idioma para nuvem de palavras:[/bold]")
            console.print("  [1] ğŸ‡·ğŸ‡º Russo")
            console.print("  [2] ğŸ‡ºğŸ‡¸ InglÃªs")

            opcao_idioma = input("\nEscolha: ").strip()
            idioma = 'ru' if opcao_idioma == '1' else 'en'

            with console.status("[cyan]Gerando nuvem de palavras..."):
                caminho = self.analisar.gerar_wordcloud_geral(idioma)

            console.mostrar_sucesso(f"Nuvem de palavras gerada em: {caminho}")

        input("\nPressione Enter para continuar...")
```

---

### 8. Presenter de AnÃ¡lise (`interface/cli/presenters_analise.py`)

**Responsabilidade:** Formatar resultados de anÃ¡lise para exibiÃ§Ã£o.

```python
class AnalisePresenter:
    """Formata resultados de anÃ¡lise para exibiÃ§Ã£o."""

    @staticmethod
    def mostrar_analise(analise):
        """Exibe anÃ¡lise completa de um documento."""
        # CabeÃ§alho
        console.print(Panel(
            f"[bold cyan]ğŸ” ANÃLISE DO DOCUMENTO {analise.documento_id}[/bold cyan]",
            border_style="cyan"
        ))

        # 1. EstatÃ­sticas bÃ¡sicas
        console.print("\n[bold]ğŸ“Š ESTATÃSTICAS DO TEXTO[/bold]")
        stats_table = Table(box=box.SIMPLE)
        stats_table.add_column("MÃ©trica", style="cyan")
        stats_table.add_column("Valor", style="white")

        e = analise.estatisticas
        stats_table.add_row("Total de caracteres", f"{e.total_caracteres:,}")
        stats_table.add_row("Total de palavras", f"{e.total_palavras:,}")
        stats_table.add_row("Total de frases", f"{e.total_frases}")
        stats_table.add_row("Palavras Ãºnicas", f"{e.palavras_unicas:,}")
        stats_table.add_row("Densidade lÃ©xica", f"{e.densidade_lexica:.2f}")

        console.print(stats_table)

        # 2. Sentimento
        console.print("\n[bold]ğŸ˜Š ANÃLISE DE SENTIMENTO[/bold]")
        s = analise.sentimento
        cor = {'positivo': 'green', 'negativo': 'red', 'neutro': 'yellow'}.get(s.classificacao, 'white')
        console.print(f"  ClassificaÃ§Ã£o: [{cor}]{s.classificacao.upper()}[/{cor}]")
        console.print(f"  Polaridade: {s.polaridade:.2f}")
        console.print(f"  Subjetividade: {s.subjetividade:.2f}")

        # 3. Entidades
        if analise.entidades_por_tipo:
            console.print("\n[bold]ğŸ·ï¸  ENTIDADES ENCONTRADAS[/bold]")
            for tipo, entidades in analise.entidades_por_tipo.items():
                if entidades:
                    console.print(f"\n  [cyan]{tipo}:[/cyan]")
                    for ent in entidades[:10]:
                        console.print(f"    â€¢ {ent}")

        # 4. Palavras mais frequentes
        if analise.palavras_frequentes:
            console.print("\n[bold]ğŸ“ˆ PALAVRAS MAIS FREQUENTES[/bold]")
            for i, (palavra, freq) in enumerate(analise.palavras_frequentes[:15], 1):
                console.print(f"  {i:2d}. {palavra}: {freq}")

        # 5. Metadados
        console.print("\n[dim]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[/dim]")
        console.print(f"[dim]Modelo: {analise.modelo_utilizado}[/dim]")
        console.print(f"[dim]Tempo: {analise.tempo_processamento:.2f}s[/dim]")

    @staticmethod
    def mostrar_estatisticas_globais(stats):
        """Exibe estatÃ­sticas globais do acervo."""
        console.print(Panel(
            "[bold cyan]ğŸ“Š ESTATÃSTICAS GLOBAIS DO ACERVO[/bold cyan]",
            border_style="cyan"
        ))

        console.print(f"\nğŸ“š Total de documentos: {stats['total_docs']}")
        console.print(f"ğŸ“ Total de palavras: {stats['total_palavras']:,}")
        console.print(f"ğŸ“Š MÃ©dia por documento: {stats['media_palavras_por_doc']:.1f}")

        console.print("\nğŸ“ DISTRIBUIÃ‡ÃƒO POR TAMANHO:")
        for cat, total in stats['documentos_por_tamanho'].items():
            console.print(f"  â€¢ {cat}: {total}")
```

---

## ğŸ”„ **Fluxo de AnÃ¡lise**

```
[UsuÃ¡rio] â†’ [Menu â†’ 6] â†’ [ComandoAnalisarDocumento] â†’ [AnalisarDocumento (caso de uso)]
    â†‘                                                    |
    |                                                    â†“
    â””â”€â”€ [Presenter] â† [AnaliseTexto] â† [SpacyAnalyzer] â† [Documento]
                                           |
                                           â†“
                                   [WordCloudGenerator]
                                           |
                                           â†“
                                   [Imagem PNG em analises/]
```

**Passo a passo (anÃ¡lise individual):**

1. UsuÃ¡rio escolhe opÃ§Ã£o 6 â†’ 1 no menu
2. Comando pergunta idioma e se quer nuvem de palavras
3. Caso de uso busca documento/traduÃ§Ã£o
4. SpacyAnalyzer carrega modelo (lazy) e analisa texto
5. EstatÃ­sticas, entidades e sentimento sÃ£o calculados
6. Se solicitado, WordCloudGenerator gera imagem
7. Resultados sÃ£o formatados pelo presenter
8. UsuÃ¡rio vÃª anÃ¡lise no terminal

---

## ğŸ® **Funcionalidades na UI**

### Menu de AnÃ¡lise:
```
ğŸ” ANÃLISE DE TEXTO
  [1] Analisar documento especÃ­fico
  [2] AnÃ¡lise global do acervo
  [3] Nuvem de palavras do acervo
  [0] Voltar
```

### AnÃ¡lise de Documento Individual:
```
ğŸ” ANÃLISE DO DOCUMENTO 1

ğŸ“Š ESTATÃSTICAS DO TEXTO
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ©trica                     â”‚ Valor   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total de caracteres         â”‚ 12,458  â”‚
â”‚ Total de palavras           â”‚ 1,832   â”‚
â”‚ Total de frases             â”‚ 124     â”‚
â”‚ Densidade lÃ©xica            â”‚ 0.42    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ˜Š ANÃLISE DE SENTIMENTO
  ClassificaÃ§Ã£o: NEUTRO
  Polaridade: 0.02

ğŸ·ï¸  ENTIDADES ENCONTRADAS
  Pessoa:
    â€¢ Ğ›.Ğ’. ĞĞ¸ĞºĞ¾Ğ»Ğ°ĞµĞ²Ğ°
    â€¢ Ğ˜.Ğ’. Ğ¡Ñ‚Ğ°Ğ»Ğ¸Ğ½
  Local:
    â€¢ Ğ›ĞµĞ½Ğ¸Ğ½Ğ³Ñ€Ğ°Ğ´

ğŸ“ˆ PALAVRAS MAIS FREQUENTES
   1. Ğ´Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: 24
   2. Ğ½Ğ¸ĞºĞ¾Ğ»Ğ°ĞµĞ²: 18
   3. Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ: 15
```

### Nuvem de Palavras:
- Gerada em `analises/wordcloud_*.png`
- VisualizaÃ§Ã£o das palavras mais frequentes

---

## ğŸ“‚ **Estrutura de Arquivos Gerados**

```
analises/
â”œâ”€â”€ wordcloud_1_ru_20260216_123456.png
â”œâ”€â”€ wordcloud_1_en_20260216_123457.png
â”œâ”€â”€ wordcloud_30_en_20260216_223916.png
â””â”€â”€ wordcloud_acervo_ru_20260216.png
```

---

## ğŸ§ª **Testes Realizados**

| Teste | AÃ§Ã£o | Resultado Esperado | Status |
|-------|------|-------------------|--------|
| AnÃ¡lise individual | 6 â†’ 1 â†’ ID â†’ ru | EstatÃ­sticas + entidades | âœ… |
| AnÃ¡lise com traduÃ§Ã£o | 6 â†’ 1 â†’ ID â†’ en | AnÃ¡lise do texto traduzido | âœ… |
| Nuvem de palavras | 6 â†’ 3 | Imagem gerada em analises/ | âœ… |
| ExtraÃ§Ã£o de entidades | Documento com nomes | Pessoas identificadas | âœ… |
| Sentimento | Texto neutro | ClassificaÃ§Ã£o neutra | âœ… |
| EstatÃ­sticas globais | 6 â†’ 2 â†’ 1 | MÃ©tricas do acervo | âœ… |

---

## ğŸ“Š **MÃ©tricas da Fase**

| MÃ©trica | Valor |
|---------|-------|
| Novos arquivos | 6 |
| Linhas de cÃ³digo | ~800 |
| Modelos de NLP | 2 (ru, en) |
| Tipos de entidades | 12+ |
| Testes manuais | 6 cenÃ¡rios |

---

## ğŸ“š **PrincÃ­pios Aplicados**

| PrincÃ­pio | AplicaÃ§Ã£o |
|-----------|-----------|
| **Lazy Loading** | Modelos carregados apenas quando necessÃ¡rios |
| **Single Responsibility** | Cada classe tem uma funÃ§Ã£o clara |
| **Open/Closed** | Novos idiomas podem ser adicionados |
| **Dependency Inversion** | Casos de uso dependem de interfaces |
| **Value Objects** | Resultados imutÃ¡veis e autocontidos |

---

## ğŸ”— **IntegraÃ§Ã£o com Fases Anteriores e Futuras**

| Fase | Relacionamento |
|------|----------------|
| **FASE 1 (Domain)** | Value Objects para anÃ¡lise |
| **FASE 2 (Application)** | Casos de uso estruturados |
| **FASE 3 (Infrastructure)** | RepositÃ³rios para dados |
| **FASE 4 (CLI)** | Comandos integrados ao menu |
| **FASE 5 (TraduÃ§Ã£o)** | AnÃ¡lise de textos traduzidos |
| **FASE 9 (Web)** | AnÃ¡lise via interface web |

---

## ğŸš€ **EvoluÃ§Ã£o do CÃ³digo**

### Antes (cÃ³digo legado)
```python
# Sem anÃ¡lise de texto
def visualizar_documento(id):
    print("Apenas o texto bruto")
```

### Depois (Clean Architecture)
```python
# AnÃ¡lise completa com NLP
analise = analyzer.analisar(texto, doc_id)
print(f"Entidades: {analise.entidades}")
print(f"Sentimento: {analise.sentimento.classificacao}")
```

---

## ğŸ“ˆ **MÃ©tricas do Projeto (ApÃ³s FASE 8)**

```
ğŸ“Š DOMAIN LAYER: 5 entidades | 18 testes
ğŸ“Š APPLICATION LAYER: 8 casos de uso | 10 testes
ğŸ“Š INFRASTRUCTURE LAYER: 6 mÃ³dulos | 20 testes
ğŸ“Š INTERFACE LAYER: 12 mÃ³dulos | Validada manualmente
ğŸ“Š TOTAL: 48 testes automatizados
```

---

## ğŸ” **LiÃ§Ãµes Aprendidas**

1. **Lazy loading Ã© essencial**: Carregar modelos sÃ³ quando usados economiza memÃ³ria
2. **Idiomas diferentes, modelos diferentes**: Cada idioma requer seu prÃ³prio modelo
3. **Fallback Ã© necessÃ¡rio**: Quando API falha, ter uma alternativa
4. **Performance importa**: AnÃ¡lise de 500 documentos leva tempo
5. **VisualizaÃ§Ãµes agregam valor**: Uma imagem vale mais que mil palavras
6. **Entidades revelam padrÃµes**: Quem sÃ£o os protagonistas dos documentos

---

## ğŸ **ConclusÃ£o da Fase**

A FASE 8 entregou um sistema completo de anÃ¡lise de texto com:

âœ… ExtraÃ§Ã£o de entidades (pessoas, locais, organizaÃ§Ãµes)
âœ… EstatÃ­sticas textuais detalhadas
âœ… AnÃ¡lise de sentimentos
âœ… GeraÃ§Ã£o de nuvens de palavras
âœ… Suporte a russo e inglÃªs
âœ… AnÃ¡lise individual e global
âœ… VisualizaÃ§Ã£o rica no terminal
âœ… Imagens em alta resoluÃ§Ã£o
âœ… CÃ³digo limpo e extensÃ­vel

---

## ğŸ‘¤ **Autor**

**Thiago Ribeiro** - Projeto de TCC

---

<div align="center">
  <sub>FASE 8 concluÃ­da em 16 de Fevereiro de 2026</sub>
  <br>
  <sub>âœ… Pronto para a FASE 9 - Web Interface</sub>
</div>
```
