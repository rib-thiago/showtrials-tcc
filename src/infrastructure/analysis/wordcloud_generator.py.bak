# src/infrastructure/analysis/wordcloud_generator.py
"""
Gerador de nuvem de palavras.
VersÃ£o robusta que aceita qualquer parÃ¢metro de configuraÃ§Ã£o.
"""

import logging
from collections import Counter
from pathlib import Path
from typing import Optional

import matplotlib.pyplot as plt
import spacy
from wordcloud import WordCloud

logger = logging.getLogger(__name__)


class WordCloudGenerator:
    """
    Gerador de nuvens de palavras a partir de textos.
    VersÃ£o flexÃ­vel que aceita configuraÃ§Ãµes via kwargs.
    """

    def __init__(self, **kwargs):
        """
        Inicializa o gerador com configuraÃ§Ãµes flexÃ­veis.

        Args:
            **kwargs: Qualquer parÃ¢metro de configuraÃ§Ã£o.
                     Os reconhecidos serÃ£o usados, outros serÃ£o ignorados.
        """
        # ParÃ¢metros reconhecidos com valores padrÃ£o
        self.default_size = kwargs.get("default_size", (800, 400))
        self.max_words = kwargs.get("max_words", 200)
        self.background_color = kwargs.get("background_color", "white")
        self.width = kwargs.get("width", self.default_size[0])
        self.height = kwargs.get("height", self.default_size[1])

        # Armazenar kwargs extras para uso futuro (ignorados)
        self._extra_kwargs = {
            k: v
            for k, v in kwargs.items()
            if k not in ["default_size", "max_words", "background_color", "width", "height"]
        }

        # Carregar stopwords para mÃºltiplos idiomas
        self.stopwords = self._carregar_stopwords()

        logger.info(
            f"ðŸ”§ WordCloudGenerator inicializado (tamanho: {self.width}x{self.height}, "
            f"max_words: {self.max_words})"
        )

        if self._extra_kwargs:
            logger.debug(f"   ParÃ¢metros extras ignorados: {list(self._extra_kwargs.keys())}")

    def _carregar_stopwords(self) -> set:
        """Carrega stopwords de vÃ¡rios idiomas."""
        stopwords = set()

        # Stopwords em russo (bÃ¡sicas)
        stopwords_ru = {
            "Ð¸",
            "Ð²",
            "Ð½Ð°",
            "Ñ",
            "Ð¿Ð¾",
            "Ð´Ð»Ñ",
            "Ñ‡Ñ‚Ð¾",
            "ÐºÐ°Ðº",
            "ÑÑ‚Ð¾",
            "Ð²ÐµÑÑŒ",
            "Ð¼Ð¾Ð¹",
            "Ñ‚Ð²Ð¾Ð¹",
            "ÐµÐ³Ð¾",
            "ÐµÐµ",
            "Ð¸Ñ…",
            "Ðº",
            "Ñƒ",
            "Ð¾",
            "Ð¸Ð·",
            "Ð·Ð°",
            "Ð½Ð°Ð´",
            "Ð¿Ð¾Ð´",
            "Ð°",
            "Ð½Ð¾",
            "Ð´Ð°",
            "Ð¸Ð»Ð¸",
            "ÐµÑÐ»Ð¸",
        }

        # Stopwords em inglÃªs
        try:
            from wordcloud import STOPWORDS

            stopwords.update(STOPWORDS)
        except:
            pass

        stopwords.update(stopwords_ru)
        return stopwords

    def gerar(
        self,
        texto: str,
        titulo: str = "Nuvem de Palavras",
        idioma: str = "ru",
        max_palavras: Optional[int] = None,
        largura: Optional[int] = None,
        altura: Optional[int] = None,
        salvar_em: Optional[str] = None,
    ) -> Optional[Path]:
        """
        Gera nuvem de palavras a partir do texto.

        Args:
            texto: Texto a ser analisado
            titulo: TÃ­tulo da imagem
            idioma: Idioma do texto ('ru', 'en', etc)
            max_palavras: NÃºmero mÃ¡ximo de palavras (sobrescreve o padrÃ£o)
            largura: Largura da imagem (sobrescreve o padrÃ£o)
            altura: Altura da imagem (sobrescreve o padrÃ£o)
            salvar_em: Caminho para salvar a imagem (se None, mostra na tela)

        Returns:
            Path da imagem salva ou None
        """
        # Usar parÃ¢metros fornecidos ou valores padrÃ£o
        max_words = max_palavras or self.max_words
        width = largura or self.width
        height = altura or self.height

        # Processar texto com SpaCy para melhor tokenizaÃ§Ã£o
        try:
            if idioma == "ru":
                nlp = spacy.load("ru_core_news_sm")
            else:
                nlp = spacy.load("en_core_web_sm")

            doc = nlp(texto[:50000])  # Limitar tamanho

            # Extrair palavras significativas
            palavras = [
                token.text.lower()
                for token in doc
                if not token.is_stop
                and not token.is_punct
                and not token.is_space
                and len(token.text) > 2
                and token.text.lower() not in self.stopwords
            ]

            # Contar frequÃªncias
            frequencias = Counter(palavras)

        except Exception as e:
            logger.warning(f"Erro ao processar com spaCy: {e}. Usando mÃ©todo simples.")
            # Fallback: split simples
            palavras = texto.lower().split()
            palavras = [p for p in palavras if len(p) > 2 and p not in self.stopwords]
            frequencias = Counter(palavras)

        if not frequencias:
            logger.warning("Nenhuma palavra significativa encontrada para gerar nuvem.")
            return None

        # Gerar nuvem
        wordcloud = WordCloud(
            width=width,
            height=height,
            background_color=self.background_color,
            max_words=max_words,
            stopwords=self.stopwords,
            collocations=False,
        ).generate_from_frequencies(frequencias)

        # Plotar
        plt.figure(figsize=(width / 100, height / 100))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.title(titulo)

        if salvar_em:
            caminho = Path(salvar_em)
            caminho.parent.mkdir(exist_ok=True)
            plt.savefig(caminho, bbox_inches="tight", dpi=300)
            plt.close()
            logger.info(f"âœ… Nuvem de palavras salva em: {caminho}")
            return caminho
        else:
            plt.show()
            return None
